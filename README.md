## Fred E

Building infrastructure for distributed AI training at scale.

Founder at **[Polaris Cloud](https://polariscloud.ai)** and **[Violet](https://github.com/useviolet)**.

### Research & Engineering

My work focuses on making large-scale model training accessible through decentralized compute:

- **Distributed Training Systems** — Building production infrastructure for training across heterogeneous, geographically distributed GPU clusters. Implementing communication-efficient parallelism strategies (SWARM, DiLoCo-style periodic averaging) that reduce bandwidth requirements while maintaining convergence.

- **Decentralized ML on Bittensor** — Core contributor at [@bit-current](https://github.com/bit-current), developing subnet infrastructure for collaborative model training. Our [DistributedTraining](https://github.com/bit-current/DistributedTraining) implementation enables fault-tolerant training across untrusted nodes.

- **Efficient Fine-tuning** — Working on [distributed DoRA training](https://github.com/bit-current/dtune) with periodic weight averaging for continual pretraining on decentralized networks.

### Selected Projects

| Project | Description |
|---------|-------------|
| [DistributedTraining](https://github.com/bit-current/DistributedTraining) | Decentralized training infrastructure on Bittensor |
| [dtune](https://github.com/bit-current/dtune) | Distributed DoRA fine-tuning with periodic weight averaging |
| [swarm](https://github.com/bit-current/swarm) | SWARM Parallelism implementation for communication-efficient training |
| [voiceflow](https://github.com/wallscaler/voiceflow) | TTS deployment platform with GPU orchestration |

### Areas

Distributed systems · Large-scale training · Communication-efficient ML · Decentralized compute · MLOps

---

Open to research collaborations. Reach me at [polariscloud.ai](https://polariscloud.ai)
